<?xml version="1.0" encoding="utf-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Noetic Nought (image_processing)</title><link>http://punchagan.muse-amuse.in/</link><description></description><atom:link href="http://punchagan.muse-amuse.in/tags/image_processing.xml" type="application/rss+xml" rel="self"></atom:link><language>en</language><lastBuildDate>Mon, 25 Nov 2013 10:38:12 GMT</lastBuildDate><generator>nikola</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Do our eyes suck at blue?</title><link>http://punchagan.muse-amuse.in/posts/do-our-eyes-suck-at-blue.html</link><description>&lt;p&gt;
This is some fun I had, trying to replicate what was written in
this post. I had been trying to understand what was happening
here, and found this post on Hacker News very helpful.
&lt;/p&gt;

&lt;p&gt;
It is a known fact that our eyes have more cones for green and
red as compared to blue.  The Bayer filter used for digital camera
lenses is based upon this principle.  This post tries to
illustrate that using the following two arguments.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Looking at only the blue channel of an image looks very dark.
&lt;/li&gt;
&lt;li&gt;Tripling the pixel size of blue channel doesn't cause much
distortion in the final image.
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;
Hence, our eyes suck at blue.
&lt;/p&gt;

&lt;p&gt;
Their argument is flawed, but we could try and improve a few
things.
&lt;/p&gt;

&lt;div id="outline-container-sec-1" class="outline-2"&gt;
&lt;h2 id="sec-1"&gt;Looking at the blue channel.&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-1"&gt;
&lt;p&gt;
This is definitely flawed, since the intensity of blue in the
image they have taken may be less and hence giving us a false
positive.
&lt;/p&gt;

&lt;p&gt;
We could instead gray-scale the image and use these pixel values
in the 3 channels and look at the images.
&lt;/p&gt;

&lt;p&gt;
This also eliminates the problem of the image being captured
through a Bayer filter.
&lt;/p&gt;

&lt;p&gt;
Here's some &lt;code&gt;python&lt;/code&gt; code to do the same. (uses &lt;code&gt;matplotlib&lt;/code&gt;)
&lt;/p&gt;
&lt;div class="org-src-container"&gt;

&lt;pre class="src src-python"&gt;def show_channels(I):
    for i in range(3):
	J = zeros_like(I)
	J[:, :, i] = I[:, :, i]
	figure(i)
	imshow(J)

def show_grey_channels(I):
    K = average(I, axis=2)
    for i in range(3):
	J = zeros_like(I)
	J[:, :, i] = K
	figure(i+10)
	imshow(J)
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-sec-2" class="outline-2"&gt;
&lt;h2 id="sec-2"&gt;Pixelating the blue channel&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-2"&gt;
&lt;p&gt;
Again, there was this argument of use of Bayer filter affecting
the image and the like.
&lt;/p&gt;

&lt;p&gt;
What I did was to swap the channels, and then look at the
images. However I swapped the channels, the image where the
green channel was pixelated always looked the worst.  The
difference between blue and red was less noticeable, I feel.
&lt;/p&gt;

&lt;p&gt;
Here's the code.
&lt;/p&gt;

&lt;div class="org-src-container"&gt;

&lt;pre class="src src-python"&gt;def zoom(x, factor=2):
    rows, cols = x.shape
    row_stride, col_stride = x.strides
    view = np.lib.stride_tricks.as_strided(x,
			(rows, factor, cols, factor),
			(row_stride, 0, col_stride, 0))
    return view.reshape((rows*factor, cols*factor))

def subsample(I):
    for i in range(3):
	J = I.copy()
	J[:, :, i] = zoom(I[::4, ::4, i], 4)
	figure(i)
	title("%s channel subsampled" %colors[i])
	imshow(J)

def swap_subsample(I, k=1):
    for c, color in enumerate(colors):
	print "%s &amp;lt;-- %s" %(colors[c], colors[(c+k)%3])
    for i in range(3):
	J = zeros_like(I)
	for j in range(3):
	    J[:, :, j] = I[:, :, (j+k)%3]
	J[:, :, i] = zoom(I[::4, ::4, (i+k)%3], 4)
	figure(i+10)
	title("%s channel subsampled" %colors[i])
	imshow(J)
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-sec-3" class="outline-2"&gt;
&lt;h2 id="sec-3"&gt;Images&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-3"&gt;
&lt;p&gt;
Here are a few images.  (View them in their original size)
&lt;/p&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="http://punchagan.muse-amuse.in/images/channels.png" alt="channels.png"&gt;
&lt;/p&gt;
&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="http://punchagan.muse-amuse.in/images/subsample.png" alt="subsample.png"&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>fun</category><category>hack</category><category>image_processing</category><category>python</category><guid>http://punchagan.muse-amuse.in/posts/do-our-eyes-suck-at-blue.html</guid><pubDate>Sat, 13 Nov 2010 07:56:00 GMT</pubDate></item><item><title>talk at GNUnify' 10</title><link>http://punchagan.muse-amuse.in/posts/talk-at-gnunify-10.html</link><description>&lt;p&gt;
Shantanu and I conducted a workshop on Scipy at GNUnify '10. It
was intended to be an introduction to Scipy and Numpy through
Image processing. We expected an audience which was python
literate. But GNUnify's schedule wasn't too favorable for us. Ours
was the first talk scheduled and we ended up getting people who
only "heard" of Python, the language.
&lt;/p&gt;

&lt;p&gt;
We had planned quite a bit of stuff expecting a python literate
audience. But unfortunately, we had to start almost from scratch
and couldn't do all of what we planned. Still, the workshop wasn't
too bad. Nobody left mid-way during the 2 hour workshop; nobody
was dozing either.  I would have liked my first talk at a FOSS
event to be better, though.
&lt;/p&gt;

&lt;p&gt;
Slides and Images that we used.  Shantanu's post on the talk is
here.
&lt;/p&gt;</description><category>image_processing</category><category>ology</category><category>python</category><category>scipy</category><guid>http://punchagan.muse-amuse.in/posts/talk-at-gnunify-10.html</guid><pubDate>Sun, 28 Feb 2010 15:28:00 GMT</pubDate></item></channel></rss>